{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sun Nov 30 11:22:29 2014\n",
    "\n",
    "@author: Marius Felix Killinger\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self):\n",
    "        self.input = None     \n",
    "    def forward(self, input):\n",
    "        return input        \n",
    "    def backward(self, top_error):\n",
    "        return top_error\n",
    "\n",
    "class Tanh(Node):\n",
    "    def __init__(self):\n",
    "        self.input = None        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.tanh(self.input)\n",
    "    def backward(self, top_error):\n",
    "        \"\"\" return d out / d in \"\"\" \n",
    "        return 1-(tanh(top_error)*tanh(top_error)) # TODO: Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PerceptronLayer(Node):\n",
    "    def __init__(self, nin, nout):\n",
    "        self.nin = nin\n",
    "        self.nout = nout\n",
    "        self.W = np.random.uniform(low=-np.sqrt(6. / (nin + nout)),\n",
    "                   high=np.sqrt(6. / (nin + nout)), size=(nin, nout)).astype(np.float32)\n",
    "        self.b = np.random.uniform(-1e-8,1e-8,(nout,)).astype(np.float32)\n",
    "     \n",
    "        self.lin = None  # stores the dot product of w and the last input\n",
    "        self.act = None     # stores the mapping of lin with the activation function\n",
    "        self.input = None # stores the last input\n",
    "        self.grad_b = None\n",
    "        self.grad_W = None\n",
    "        self.act_func = Tanh()\n",
    "\n",
    "                    \n",
    "    def forward(self, input):\n",
    "        \"\"\" (bs, n_in) --> (bs, n_out) \"\"\"\n",
    "        self.lin = np.dot(self.W,input)\n",
    "        self.act = self.act_func.forward(self.lin)\n",
    "        self.input = input\n",
    "        return self.act     \n",
    "\n",
    "    def backward(self, top_error):\n",
    "        \"\"\" d out / d in \"\"\" \n",
    "        self.act_error = self.act_func.backward(top_error)\n",
    "        err = self.act_error*self.input\n",
    "        return err\n",
    "    \n",
    "    def grad(self, top_error):\n",
    "        \"\"\" d out / d W \"\"\" \n",
    "        grad_b = np.gradient(self.b)\n",
    "        grad_W = np.gradient(self.W)\n",
    "        self.grad_b = grad_b\n",
    "        self.grad_W = grad_W\n",
    "        return grad_W, grad_b\n",
    "        \n",
    "    def GD_update(self, lr):\n",
    "        self.W = self.W - (lr*self.grad_W)\n",
    "        self.b = self.b - (lr*self.grad_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Softmax(Node):\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        \"\"\" return softmax function to input vector\"\"\"\n",
    "        return np.exp(self.input) / np.sum(np.exp(self.input), axis=0)\n",
    "        \n",
    "    def backward(self, top_error):\n",
    "    \t\"\"\" return the back propagation error \"\"\"\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NLL(object):\n",
    "    def __init__(self, n_lab):\n",
    "        self.input   = None\n",
    "        self.classes = np.arange(n_lab, dtype=np.int)[None,:]\n",
    "\n",
    "    def forward(self, input, Y):\n",
    "        self.input = input\n",
    "        self.n     = Y.shape[0]\n",
    "        self.active_class = np.equal(self.classes,Y[:,None])\n",
    "        return np.sum(not self.active_class)/self.n\n",
    "\n",
    "    def backward(self):\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, layer_sizes, nin):\n",
    "        self.layers         = []\n",
    "        self.last_grads = None\n",
    "        n_lay                     = len(layer_sizes)\n",
    "        for i in xrange(n_lay-1):\n",
    "            print \"Adding layer (#in %i, #out %i)\" % (nin, layer_sizes[i])\n",
    "            self.layers.append(PerceptronLayer(nin, layer_sizes[i]))\n",
    "            nin = layer_sizes[i]\n",
    "            \n",
    "        print \"Adding layer (#in %i, #out %i)\" % (nin, layer_sizes[-1])    \n",
    "        self.layers.append(PerceptronLayer(nin,layer_sizes[-1], act_func='lin'))\n",
    "        \n",
    "        self.softmax = Softmax()\n",
    "        self.loss = NLL(layer_sizes[-1])\n",
    "        \n",
    "    def forward(self, X):\n",
    "        bs = X.shape[0]\n",
    "        X = X.reshape(bs, -1)\n",
    "        result = X\n",
    "        for lay in self.layers:\n",
    "            result = lay.forward(result)\n",
    "        return result\n",
    "        \n",
    "    def class_prob(self, X):\n",
    "        out = self.forward(X) \n",
    "        return self.softmax.forward(out)\n",
    "         \n",
    "    def get_loss(self, X, Y):\n",
    "        pred = self.class_prob(X)\n",
    "        loss = self.loss.forward(pred, Y)\n",
    "        cls = pred.argmax(axis=1)\n",
    "        acc = 1-np.mean(np.equal(cls, Y))\n",
    "        return loss, acc\n",
    "        \n",
    "        \n",
    "    def gradients(self, X, Y):\n",
    "        class_prob = self.class_prob(X)\n",
    "        loss = self.loss.forward(class_prob, Y)\n",
    "        \n",
    "        top_err = self.loss.backward()\n",
    "        top_err = self.softmax.backward(top_err)\n",
    "        grads = []\n",
    "        for lay in self.layers[::-1]:\n",
    "            new_err = lay.backward(top_err)\n",
    "            grad_W, grad_b = lay.grad(top_err)\n",
    "            grads.append(grad_b)\n",
    "            grads.append(grad_W)\n",
    "            top_err = new_err\n",
    "        self.last_grads = grads\n",
    "        return grads[::-1], loss, class_prob\n",
    "        \n",
    "    def update(self, lr):\n",
    "        for lay in self.layers:\n",
    "            lay.GD_update(lr)\n",
    "\n",
    "            \n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    n = 2000\n",
    "    X, Y = datasets.make_moons(n, noise=0.05)\n",
    "    X_test   = np.meshgrid(np.linspace(-1,1,20), np.linspace(-1,1,20))\n",
    "    X_test   = np.vstack((X_test[0].flatten(), X_test[1].flatten())).T\n",
    "    X -= X.min(axis=0)\n",
    "    X /= X.max(axis=0)\n",
    "    X  = (X - 0.5) * 2\n",
    "\n",
    "    nin = 2\n",
    "    bs  = 200\n",
    "    lr  = 0.05\n",
    "\n",
    "    trace = dict(X=X, Y=Y, W1=[], b1=[], a1=[], W2=[], b2=[], a2=[], dec=[])\n",
    "    nn = MLP([3,2], nin)\n",
    "\n",
    "    pos = 0\n",
    "    perm = np.random.permutation(n)\n",
    "\n",
    "    nn = MLP([3,3,10], nin)\n",
    "    for i in xrange(10000):\n",
    "    grads, loss, pred = nn.gradients(X[perm[pos:pos+bs]], Y[perm[pos:pos+bs]])\n",
    "    nn.update(lr)\n",
    "    if i%1000==0:\n",
    "        valid_loss, valid_err = nn.get_loss(X, Y)\n",
    "        print \"Loss:\",loss,\"Valid Loss:\",valid_loss,\"Valid Error:\",valid_err"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
